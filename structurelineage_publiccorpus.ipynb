{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbyd8ncBmgxUjMX5nWGd+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e116181217948c2b4b20d48d01c0128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27b30b9fd38e4a74bae3f96fdbe680e7",
              "IPY_MODEL_8700ac9fa5ff4bbd9856ab6007afa163",
              "IPY_MODEL_62b6c6b3fcdf43e9bd7e4c6ba18ea47a"
            ],
            "layout": "IPY_MODEL_0fbcb47803274b10812311e22e8b938f"
          }
        },
        "27b30b9fd38e4a74bae3f96fdbe680e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d00fa2e4f934bbb80b9fc2738be6e48",
            "placeholder": "​",
            "style": "IPY_MODEL_8d92ee5b0db74c72ae997d0754c98ddf",
            "value": "repos: 100%"
          }
        },
        "8700ac9fa5ff4bbd9856ab6007afa163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d37f0300144025aeae1dc5b09e760b",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e993789bde844ca790c53aa7e5d31536",
            "value": 13
          }
        },
        "62b6c6b3fcdf43e9bd7e4c6ba18ea47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_becb1daf4a2c446dbe0c2fdcb2c95b7a",
            "placeholder": "​",
            "style": "IPY_MODEL_f36c14b520024f49bf293477f927c0c2",
            "value": " 13/13 [00:00&lt;00:00, 330.15it/s]"
          }
        },
        "0fbcb47803274b10812311e22e8b938f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d00fa2e4f934bbb80b9fc2738be6e48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d92ee5b0db74c72ae997d0754c98ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d37f0300144025aeae1dc5b09e760b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e993789bde844ca790c53aa7e5d31536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "becb1daf4a2c446dbe0c2fdcb2c95b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36c14b520024f49bf293477f927c0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habiiibo03/StructureLineage/blob/main/structurelineage_publiccorpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nTyWhiF_3MoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d12ba55-285f-4f49-81c3-512a83b5fbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StructureLineage'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 75 (delta 16), reused 48 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (75/75), 65.18 KiB | 3.43 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "/content/StructureLineage\n",
            "Requirement already satisfied: sqlglot>=11.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (25.20.2)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (5.10.4)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (8.4.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.2.0->-r requirements.txt (line 3)) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.2.0->-r requirements.txt (line 3)) (4.25.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.2.0->-r requirements.txt (line 3)) (5.9.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.2.0->-r requirements.txt (line 3)) (5.7.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->-r requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->-r requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->-r requirements.txt (line 7)) (2.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->-r requirements.txt (line 3)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->-r requirements.txt (line 3)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->-r requirements.txt (line 3)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->-r requirements.txt (line 3)) (0.28.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.2.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat>=5.2.0->-r requirements.txt (line 3)) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/habiiibo03/StructureLineage.git  # if not already cloned\n",
        "%cd StructureLineage\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Full corpus collection + SDG parsing + DuckDB probing + push ------------------\n",
        "# Paste/run as a single cell in Colab. Requires Colab Secrets: GITHUB_TOKEN, GITHUB_USERNAME\n",
        "import os, sys, time, json, csv, subprocess, shutil, traceback\n",
        "from pathlib import Path\n",
        "from urllib.parse import quote_plus\n",
        "from typing import List, Dict, Any\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "QUERY = \"topic:dbt language:SQL\"    # modify as you wish\n",
        "PAGES = 2\n",
        "PER_PAGE = 30\n",
        "MAX_REPOS = 40                      # safety cap\n",
        "OUT_DIR = \"public_repos\"\n",
        "CLONE_DEPTH = 1\n",
        "SL_SRC_PATH = \"/content/StructureLineage/src\"  # path to your code\n",
        "PROBE_SAMPLE_LIMIT = 5000           # number of rows to sample for probing (max)\n",
        "TOP_EDGES_PER_REPO = 10             # how many top edges to attempt probing / sample\n",
        "PUSH_BACK_TO_GITHUB = True          # set False to skip pushing summary back to GitHub\n",
        "GITHUB_REPO_TO_PUSH = f\"{userdata.get('GITHUB_USERNAME')}/StructureLineage\"  # where to push summary\n",
        "\n",
        "# -------------------- Sanity checks --------------------\n",
        "if SL_SRC_PATH not in sys.path:\n",
        "    sys.path.append(SL_SRC_PATH)\n",
        "\n",
        "try:\n",
        "    from src.sl_core.build_sdg import process_workspace\n",
        "    from src.sl_core.graph_store import SDGStore\n",
        "except Exception as e:\n",
        "    # fallback import if run differently\n",
        "    try:\n",
        "        from sl_core.build_sdg import process_workspace\n",
        "        from sl_core.graph_store import SDGStore\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Cannot import StructureLineage code. Ensure /content/StructureLineage/src exists and contains src/sl_core.\") from e\n",
        "\n",
        "# -------------------- Helpers --------------------\n",
        "def get_secrets():\n",
        "    token = userdata.get('GITHUB_TOKEN')\n",
        "    user = userdata.get('GITHUB_USERNAME')\n",
        "    if not token or not user:\n",
        "        raise RuntimeError(\"Set Colab secrets GITHUB_TOKEN and GITHUB_USERNAME before running.\")\n",
        "    return token, user\n",
        "\n",
        "def search_github_repo_urls(query: str, token: str, pages: int=1, per_page:int=30) -> List[str]:\n",
        "    headers = {'Authorization': f'Bearer {token}', 'Accept': 'application/vnd.github+json'}\n",
        "    repos = []\n",
        "    for page in range(1, pages+1):\n",
        "        url = f\"https://api.github.com/search/repositories?q={quote_plus(query)}&per_page={per_page}&page={page}\"\n",
        "        r = requests.get(url, headers=headers)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(f\"GitHub API error {r.status_code}: {r.text}\")\n",
        "        js = r.json()\n",
        "        for it in js.get('items',[]):\n",
        "            repos.append(it['html_url'])\n",
        "    # deduplicate & cap\n",
        "    unique=[]\n",
        "    for r in repos:\n",
        "        if r not in unique:\n",
        "            unique.append(r)\n",
        "    return unique[:MAX_REPOS]\n",
        "\n",
        "def safe_clone(repo_url: str, dest_dir: str, depth: int=1) -> bool:\n",
        "    if os.path.exists(dest_dir):\n",
        "        return True\n",
        "    cmd = [\"git\", \"clone\", \"--depth\", str(depth), repo_url, dest_dir]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=300)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[clone error] {repo_url}: {str(e)[:300]}\")\n",
        "        return False\n",
        "\n",
        "def count_files(root: str):\n",
        "    sql=ipynb=py=csv_files=parquet_files=0\n",
        "    for dirpath, dirs, files in os.walk(root):\n",
        "        # ignore large folders\n",
        "        if any(x in dirpath for x in [\"node_modules\", \".venv\", \".git\", \"__pycache__\"]):\n",
        "            continue\n",
        "        for f in files:\n",
        "            lf = f.lower()\n",
        "            if lf.endswith(\".sql\"): sql += 1\n",
        "            elif lf.endswith(\".ipynb\"): ipynb += 1\n",
        "            elif lf.endswith(\".py\"): py += 1\n",
        "            elif lf.endswith(\".csv\"): csv_files += 1\n",
        "            elif lf.endswith(\".parquet\") or lf.endswith(\".pq\"): parquet_files += 1\n",
        "    return {\"sql\": sql, \"ipynb\": ipynb, \"py\": py, \"csv\": csv_files, \"parquet\": parquet_files}\n",
        "\n",
        "def read_sdg_json(path: str) -> Dict:\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    with open(path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json(obj, path: str):\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "# Probe helpers using DuckDB (safe: local file reads only)\n",
        "def register_local_files_in_duckdb(con: duckdb.DuckDBPyConnection, repo_dir: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    For each CSV/parquet file found, create a temporary table in DuckDB and return mapping table_name->file_path.\n",
        "    Table names are sanitized from filenames.\n",
        "    \"\"\"\n",
        "    table_map = {}\n",
        "    for root, dirs, files in os.walk(repo_dir):\n",
        "        if any(x in root for x in [\"node_modules\", \".venv\", \".git\", \"__pycache__\"]):\n",
        "            continue\n",
        "        for f in files:\n",
        "            lf = f.lower()\n",
        "            if lf.endswith(\".csv\") or lf.endswith(\".parquet\") or lf.endswith(\".pq\"):\n",
        "                full = os.path.join(root, f)\n",
        "                tbl = \"t_\" + os.path.splitext(f)[0].replace('-', '_').replace('.', '_').replace(' ', '_')\n",
        "                # ensure unique table name\n",
        "                i = 1\n",
        "                base = tbl\n",
        "                while tbl in table_map:\n",
        "                    tbl = f\"{base}_{i}\"; i += 1\n",
        "                try:\n",
        "                    if lf.endswith(\".csv\"):\n",
        "                        con.execute(f\"CREATE TABLE {tbl} AS SELECT * FROM read_csv_auto('{full}') LIMIT {PROBE_SAMPLE_LIMIT}\")\n",
        "                    else:\n",
        "                        con.execute(f\"CREATE TABLE {tbl} AS SELECT * FROM '{full}' LIMIT {PROBE_SAMPLE_LIMIT}\")\n",
        "                    table_map[tbl] = full\n",
        "                except Exception as e:\n",
        "                    # skip problematic files\n",
        "                    # print(f\"[duckdb register error] {full}: {e}\")\n",
        "                    pass\n",
        "    return table_map\n",
        "\n",
        "def probe_pair_in_duckdb(con: duckdb.DuckDBPyConnection, left_expr: str, right_expr: str, from_clause: str, limit=2000):\n",
        "    \"\"\"\n",
        "    left_expr and right_expr are qualified column expressions like \"t.col\" or just \"col\".\n",
        "    from_clause is a FROM ... (tables/views) clause usable in DuckDB.\n",
        "    Returns overlap fraction in [0,1] or None on failure.\n",
        "    \"\"\"\n",
        "    q = f\"SELECT {left_expr} as left_col, {right_expr} as right_col FROM {from_clause} LIMIT {limit}\"\n",
        "    try:\n",
        "        df = con.execute(q).fetchdf()\n",
        "    except Exception:\n",
        "        return None\n",
        "    if df.empty:\n",
        "        return 0.0\n",
        "    left = df['left_col'].dropna()\n",
        "    right_set = set(df['right_col'].dropna().unique())\n",
        "    if left.empty or len(right_set)==0:\n",
        "        return 0.0\n",
        "    matches = left.apply(lambda v: v in right_set).sum()\n",
        "    return float(matches) / float(len(left))\n",
        "\n",
        "def attempt_probe_for_edge(con: duckdb.DuckDBPyConnection, table_map: Dict[str,str], edge_src: str, edge_tgt: str):\n",
        "    \"\"\"\n",
        "    edge_src / edge_tgt formatted like \"schema.table.col\" or \"table.col\" or \"col\".\n",
        "    We'll try to find tables in table_map that contain those columns and probe overlap.\n",
        "    Return probe_score or None if cannot probe.\n",
        "    \"\"\"\n",
        "    def last_token(x):\n",
        "        return x.split('.')[-1]\n",
        "    src_col = last_token(edge_src)\n",
        "    tgt_col = last_token(edge_tgt)\n",
        "    candidate_tables = []\n",
        "    # find tables that have these columns\n",
        "    for tbl in table_map.keys():\n",
        "        try:\n",
        "            cols = [c.lower() for c in con.execute(f\"PRAGMA table_info('{tbl}')\").fetchdf()['column_name'].tolist()]\n",
        "        except Exception:\n",
        "            cols = []\n",
        "        if src_col.lower() in cols:\n",
        "            candidate_tables.append((tbl, 'src'))\n",
        "        if tgt_col.lower() in cols:\n",
        "            candidate_tables.append((tbl, 'tgt'))\n",
        "    # map sets\n",
        "    src_tables = [t for t,role in candidate_tables if role=='src' or (role=='src' and True)]\n",
        "    tgt_tables = [t for t,role in candidate_tables if role=='tgt' or (role=='tgt' and True)]\n",
        "    src_tables = list({t for t,role in candidate_tables if role=='src'} )\n",
        "    tgt_tables = list({t for t,role in candidate_tables if role=='tgt'} )\n",
        "    # if same table contains both columns\n",
        "    for t in src_tables:\n",
        "        if t in tgt_tables:\n",
        "            # same table probe\n",
        "            left_expr = f'\"{src_col}\"'\n",
        "            right_expr = f'\"{tgt_col}\"'\n",
        "            from_clause = f'\"{t}\"'\n",
        "            score = probe_pair_in_duckdb(con, left_expr, right_expr, from_clause, limit=PROBE_SAMPLE_LIMIT)\n",
        "            if score is not None:\n",
        "                return score, f\"{t}.{src_col} <-> {t}.{tgt_col}\"\n",
        "    # otherwise try cross-table join on equality for each pair\n",
        "    for s in src_tables:\n",
        "        for t in tgt_tables:\n",
        "            left_expr = f'\"{s}\".\"{src_col}\"'\n",
        "            right_expr = f'\"{t}\".\"{tgt_col}\"'\n",
        "            from_clause = f'\"{s}\" JOIN \"{t}\" ON \"{s}\".\"{src_col}\" = \"{t}\".\"{tgt_col}\"'\n",
        "            score = probe_pair_in_duckdb(con, left_expr, right_expr, from_clause, limit=PROBE_SAMPLE_LIMIT)\n",
        "            if score is not None:\n",
        "                return score, f\"{s}.{src_col} JOIN {t}.{tgt_col}\"\n",
        "    return None, None\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "token, gh_user = get_secrets()\n",
        "print(\"GitHub user:\", gh_user)\n",
        "print(\"Search query:\", QUERY)\n",
        "\n",
        "repos = search_github_repo_urls(QUERY, token, pages=PAGES, per_page=PER_PAGE)\n",
        "print(f\"Found {len(repos)} candidates (capped at MAX_REPOS={MAX_REPOS})\")\n",
        "repos = repos[:MAX_REPOS]\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "with open(os.path.join(OUT_DIR, \"repos.json\"), \"w\") as f:\n",
        "    json.dump(repos, f, indent=2)\n",
        "\n",
        "summary = []\n",
        "failed = []\n",
        "start = time.time()\n",
        "\n",
        "for repo_url in tqdm(repos, desc=\"repos\"):\n",
        "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
        "    target_dir = os.path.join(OUT_DIR, repo_name)\n",
        "    ok = safe_clone(repo_url, target_dir, depth=CLONE_DEPTH)\n",
        "    if not ok:\n",
        "        failed.append({\"repo\": repo_url, \"reason\": \"clone_failed\"})\n",
        "        continue\n",
        "    counts = count_files(target_dir)\n",
        "\n",
        "    # parse workspace (safe parsing only)\n",
        "    try:\n",
        "        t0 = time.time()\n",
        "        store = process_workspace(target_dir, catalog=None, calibrator=None)\n",
        "        parse_time = round(time.time() - t0, 2)\n",
        "        # persist raw SDG\n",
        "        repo_sdg_path = os.path.join(target_dir, \"sdg.json\")\n",
        "        store.persist_json(repo_sdg_path)\n",
        "    except Exception as e:\n",
        "        parse_time = None\n",
        "        failed.append({\"repo\": repo_url, \"reason\": \"parse_failed\", \"error\": str(e)[:400]})\n",
        "        summary.append({\"repo\": repo_url, \"dir\": target_dir, \"counts\": counts, \"parse_time_s\": parse_time, \"sdg\": None, \"error\": str(e)[:400]})\n",
        "        continue\n",
        "\n",
        "    # extract top edges for manual sampling\n",
        "    # edges stored as list of dicts as persisted in sdg.json\n",
        "    raw_sdg = read_sdg_json(repo_sdg_path)\n",
        "    edges = raw_sdg.get(\"edges\", [])\n",
        "    # take top by prob\n",
        "    edges_sorted = sorted(edges, key=lambda e: float(e.get(\"prob\",0.0)), reverse=True)\n",
        "    top_edges = edges_sorted[:TOP_EDGES_PER_REPO]\n",
        "\n",
        "    # Probe using DuckDB if local CSV/Parquet files exist\n",
        "    probe_results = []\n",
        "    table_map = {}\n",
        "    if counts.get(\"csv\",0) + counts.get(\"parquet\",0) > 0:\n",
        "        # create duckdb connection\n",
        "        con = duckdb.connect(database=':memory:')\n",
        "        table_map = register_local_files_in_duckdb(con, target_dir)\n",
        "        if table_map:\n",
        "            # try probing each top edge\n",
        "            for e in top_edges:\n",
        "                src = e.get(\"src\")\n",
        "                tgt = e.get(\"tgt\")\n",
        "                prob_before = float(e.get(\"prob\", 0.0))\n",
        "                score, probe_desc = attempt_probe_for_edge(con, table_map, src, tgt)\n",
        "                if score is not None:\n",
        "                    # combine probabilistically: p_new = 1 - (1-p)*(1-probe)\n",
        "                    updated_prob = 1.0 - (1.0 - prob_before) * (1.0 - float(score))\n",
        "                else:\n",
        "                    updated_prob = prob_before\n",
        "                # collect probe metadata\n",
        "                probe_results.append({\"src\": src, \"tgt\": tgt, \"prob_before\": prob_before, \"probe_score\": score, \"probe_desc\": probe_desc, \"prob_after\": updated_prob})\n",
        "                # annotate the edge in raw_sdg edges (best-effort)\n",
        "                e[\"probe_score\"] = score\n",
        "                e[\"probe_desc\"] = probe_desc\n",
        "                e[\"prob_after_probe\"] = updated_prob\n",
        "        else:\n",
        "            # no registerable files found\n",
        "            pass\n",
        "\n",
        "    # persist sdg with probe info\n",
        "    repo_sdg_with_probes = os.path.join(target_dir, \"sdg_with_probes.json\")\n",
        "    save_json(raw_sdg, repo_sdg_with_probes)\n",
        "\n",
        "    # human friendly sample output (print top 5)\n",
        "    print(\"\\n----------------------------------------------\")\n",
        "    print(f\"Repo: {repo_url}\")\n",
        "    print(f\"Files: {counts}, parse_time_s: {parse_time}, registered_tables: {len(table_map)}\")\n",
        "    print(\"Top edges (sample):\")\n",
        "    for idx, e in enumerate(top_edges[:5], start=1):\n",
        "        print(f\" {idx}. {e.get('src')} -> {e.get('tgt')}  prob={e.get('prob',0.0):.3f} probe={e.get('probe_score')} desc={e.get('probe_desc')}\")\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    # add to summary\n",
        "    sdg_stats = {\n",
        "        \"num_nodes\": raw_sdg.get(\"nodes\", []).__len__(),\n",
        "        \"num_edges\": len(edges),\n",
        "        \"avg_prob\": (sum(float(x.get(\"prob\",0.0)) for x in edges) / len(edges)) if edges else 0.0\n",
        "    }\n",
        "    summary.append({\"repo\": repo_url, \"dir\": target_dir, \"counts\": counts, \"parse_time_s\": parse_time, \"sdg\": sdg_stats, \"probe_samples\": probe_results})\n",
        "\n",
        "# Save aggregated outputs\n",
        "agg = {\n",
        "    \"query\": QUERY,\n",
        "    \"date_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
        "    \"repos_found\": len(repos),\n",
        "    \"repos_processed\": len(summary),\n",
        "    \"repos_failed\": failed,\n",
        "    \"total_sql\": sum(x[\"counts\"][\"sql\"] for x in summary),\n",
        "    \"total_ipynb\": sum(x[\"counts\"][\"ipynb\"] for x in summary),\n",
        "    \"total_edges\": sum(x[\"sdg\"][\"num_edges\"] if x[\"sdg\"] is not None else 0 for x in summary),\n",
        "    \"avg_edges_per_repo\": (sum(x[\"sdg\"][\"num_edges\"] if x[\"sdg\"] is not None else 0 for x in summary) / len(summary)) if summary else 0.0,\n",
        "    \"per_repo\": summary\n",
        "}\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "agg_path = os.path.join(OUT_DIR, \"corpus_summary_with_probes.json\")\n",
        "save_json(agg, agg_path)\n",
        "# write CSV\n",
        "csv_path = os.path.join(OUT_DIR, \"corpus_summary_with_probes.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as cf:\n",
        "    writer = csv.writer(cf)\n",
        "    writer.writerow([\"repo\",\"dir\",\"sql_count\",\"ipynb_count\",\"py_count\",\"csv_count\",\"parquet_count\",\"parse_time_s\",\"num_edges\",\"avg_prob\"])\n",
        "    for r in summary:\n",
        "        c = r[\"counts\"]\n",
        "        writer.writerow([r[\"repo\"], r[\"dir\"], c[\"sql\"], c[\"ipynb\"], c[\"py\"], c[\"csv\"], c[\"parquet\"], r.get(\"parse_time_s\"), r[\"sdg\"][\"num_edges\"] if r[\"sdg\"] is not None else 0, r[\"sdg\"][\"avg_prob\"] if r[\"sdg\"] is not None else 0.0])\n",
        "\n",
        "print(\"\\n--- Done. Summary saved to:\", agg_path, \"and\", csv_path)\n",
        "\n",
        "# -------------------- Optional: push summary back to your GitHub repo --------------------\n",
        "if PUSH_BACK_TO_GITHUB:\n",
        "    try:\n",
        "        token, username = get_secrets()\n",
        "        # set remote using token for secure push\n",
        "        repo_local_path = \"/content/StructureLineage\"\n",
        "        if os.path.exists(repo_local_path):\n",
        "            os.chdir(repo_local_path)\n",
        "            # set git identity (safe)\n",
        "            subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", username])\n",
        "            subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", f\"{username}@users.noreply.github.com\"])\n",
        "            remote_url = f\"https://{username}:{token}@github.com/{username}/StructureLineage.git\"\n",
        "            subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", remote_url], check=False)\n",
        "            # copy agg files into repo under /data/corpus_runs/<timestamp> for archiving\n",
        "            archive_dir = os.path.join(repo_local_path, \"data\", \"corpus_runs\", time.strftime(\"%Y%m%d_%H%M%S\"))\n",
        "            os.makedirs(archive_dir, exist_ok=True)\n",
        "            shutil.copy(agg_path, os.path.join(archive_dir, os.path.basename(agg_path)))\n",
        "            shutil.copy(csv_path, os.path.join(archive_dir, os.path.basename(csv_path)))\n",
        "            subprocess.run([\"git\", \"add\", \".\"], check=False)\n",
        "            subprocess.run([\"git\", \"commit\", \"-m\", f\"Add corpus summary run {time.strftime('%Y%m%d_%H%M%S')}\"], check=False)\n",
        "            push = subprocess.run([\"git\", \"push\", \"origin\", \"main\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
        "            print(\"Git push stdout:\", push.stdout.decode(errors='ignore')[:1000])\n",
        "            print(\"Git push stderr:\", push.stderr.decode(errors='ignore')[:1000])\n",
        "            print(\"Summary pushed to your StructureLineage repo under data/corpus_runs/\")\n",
        "        else:\n",
        "            print(\"StructureLineage local repo not found at\", repo_local_path, \"; skipping push.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to push summary to GitHub:\", str(e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"All finished.\")\n",
        "# -------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "6e116181217948c2b4b20d48d01c0128",
            "27b30b9fd38e4a74bae3f96fdbe680e7",
            "8700ac9fa5ff4bbd9856ab6007afa163",
            "62b6c6b3fcdf43e9bd7e4c6ba18ea47a",
            "0fbcb47803274b10812311e22e8b938f",
            "6d00fa2e4f934bbb80b9fc2738be6e48",
            "8d92ee5b0db74c72ae997d0754c98ddf",
            "91d37f0300144025aeae1dc5b09e760b",
            "e993789bde844ca790c53aa7e5d31536",
            "becb1daf4a2c446dbe0c2fdcb2c95b7a",
            "f36c14b520024f49bf293477f927c0c2"
          ]
        },
        "id": "tiJXmpmy37Tv",
        "outputId": "4b4e73b9-5339-4374-ea0a-525c820ca831"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub user: habiiibo03\n",
            "Search query: topic:dbt language:SQL\n",
            "Found 13 candidates (capped at MAX_REPOS=40)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "repos:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e116181217948c2b4b20d48d01c0128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Done. Summary saved to: public_repos/corpus_summary_with_probes.json and public_repos/corpus_summary_with_probes.csv\n",
            "Git push stdout: \n",
            "Git push stderr: To https://github.com/habiiibo03/StructureLineage.git\n",
            "   7aa8415..1e7ff44  main -> main\n",
            "\n",
            "Summary pushed to your StructureLineage repo under data/corpus_runs/\n",
            "All finished.\n"
          ]
        }
      ]
    }
  ]
}